import java.io.IOException;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class FileCount {

    
 public static class FilCount extends Mapper<Object, Text, Text, IntWritable>
 {

private final static IntWritable one = new IntWritable(1);
private Text State = new Text();
     
 public void map(Object key, Text value, Context context) throws IOException, InterruptedException 
 {
 
   String line = value.toString();
   String[] elements = line.split(",");
   
   String val = elements[0];
   
   State = new Text(elements[0]);
   
   
   if ( val.equals("Onida"))
   {       
    State= new Text(elements[3]);           
    context.write(State, one);
   }

 }
 }
 
 public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> 
 {
private IntWritable result = new IntWritable();
public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {

int sum = 0;
for (IntWritable val : values) {
  sum += val.get();
}

result.set(sum);

context.write(key, result);

}

}

public static void main(String[] args) throws Exception
{
    Configuration conf = new Configuration();

 Job job = Job.getInstance(conf, "Aggrigate Data");

 job.setJarByClass(FileCount.class);

 job.setMapperClass(FilCount.class);
 job.setReducerClass(IntSumReducer.class);
 
 job.setOutputKeyClass(Text.class);
 job.setOutputValueClass(IntWritable.class);

 FileInputFormat.addInputPath(job, new Path(args[0]));
 FileOutputFormat.setOutputPath(job, new Path(args[1]));

 System.exit(job.waitForCompletion(true) ? 0 : 1);

}

 
}